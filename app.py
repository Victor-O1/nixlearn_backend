"""
Mr. Dexter - Master Teacher Backend
Compact FastAPI server with smart LLM-based chunking and RAG
"""
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List
import os, json, uuid, shutil
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()
# Document processing
import pdfplumber
from docx import Document as DocxDocument
from pptx import Presentation

# LLM & RAG
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

print("\n" + "="*80)
print("ğŸ“ MR. DEXTER - MASTER TEACHER BACKEND")
print("="*80)

# Initialize FastAPI
app = FastAPI(title="Mr. Dexter API")
print("âœ… [INIT] FastAPI app initialized")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3002"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
print("âœ… [INIT] CORS middleware configured")

# Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
print(f"ğŸ”‘ [INIT] Groq API key loaded: {'âœ… YES' if GROQ_API_KEY else 'âŒ NO (SET GROQ_API_KEY!)'}")

UPLOAD_DIR = Path("./uploads")
SESSIONS_DIR = Path("./sessions")
UPLOAD_DIR.mkdir(exist_ok=True)
SESSIONS_DIR.mkdir(exist_ok=True)
print(f"ğŸ“ [INIT] Upload directory: {UPLOAD_DIR.absolute()}")
print(f"ğŸ’¾ [INIT] Sessions directory: {SESSIONS_DIR.absolute()}")

# Initialize LLM
print("ğŸ¤– [INIT] Initializing Groq LLM...")
llm = ChatGroq(
    groq_api_key=GROQ_API_KEY,
    model_name="llama-3.3-70b-versatile",
    temperature=0.3
)
print("âœ… [INIT] Groq LLM ready (model: llama-3.3-70b-versatile, temp: 0.3)")

# Initialize embeddings
print("ğŸ§¬ [INIT] Initializing embeddings model...")
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
print("âœ… [INIT] Embeddings model ready (all-MiniLM-L6-v2)")

# Models
class SessionCreate(BaseModel):
    syllabus_text: Optional[str] = None

class ChunkResponse(BaseModel):
    chunk_id: str
    topic: str
    content: str
    summary: str
    order: int

class SessionResponse(BaseModel):
    session_id: str
    chunks: List[ChunkResponse]
    syllabus_topics: Optional[List[str]] = None

# ========== DOCUMENT EXTRACTION ==========

def extract_text_from_pdf(file_path: str) -> str:
    print(f"\nğŸ“„ [EXTRACT-PDF] Starting PDF extraction")
    print(f"ğŸ“„ [EXTRACT-PDF] File: {file_path}")
    text = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            total_pages = len(pdf.pages)
            print(f"ğŸ“„ [EXTRACT-PDF] Total pages: {total_pages}")
            
            for i, page in enumerate(pdf.pages, 1):
                print(f"ğŸ“„ [EXTRACT-PDF] Processing page {i}/{total_pages}...")
                page_text = page.extract_text() or ""
                text += page_text + "\n\n"
                print(f"ğŸ“„ [EXTRACT-PDF] Page {i}: {len(page_text)} chars extracted")
                
        print(f"ğŸ“„ [EXTRACT-PDF] âœ… Complete! Total text: {len(text)} chars")
        print(f"ğŸ“„ [EXTRACT-PDF] Preview: {text[:200]}...")
        return text
    except Exception as e:
        print(f"ğŸ“„ [EXTRACT-PDF] âŒ ERROR: {e}")
        raise

def extract_text_from_docx(file_path: str) -> str:
    print(f"\nğŸ“ [EXTRACT-DOCX] Starting DOCX extraction")
    print(f"ğŸ“ [EXTRACT-DOCX] File: {file_path}")
    try:
        doc = DocxDocument(file_path)
        total_paras = len(doc.paragraphs)
        print(f"ğŸ“ [EXTRACT-DOCX] Total paragraphs: {total_paras}")
        
        text = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        print(f"ğŸ“ [EXTRACT-DOCX] âœ… Complete! Total text: {len(text)} chars")
        print(f"ğŸ“ [EXTRACT-DOCX] Preview: {text[:200]}...")
        return text
    except Exception as e:
        print(f"ğŸ“ [EXTRACT-DOCX] âŒ ERROR: {e}")
        raise

def extract_text_from_pptx(file_path: str) -> str:
    print(f"\nğŸ¯ [EXTRACT-PPTX] Starting PPTX extraction")
    print(f"ğŸ¯ [EXTRACT-PPTX] File: {file_path}")
    try:
        prs = Presentation(file_path)
        total_slides = len(prs.slides)
        print(f"ğŸ¯ [EXTRACT-PPTX] Total slides: {total_slides}")
        
        text = ""
        for i, slide in enumerate(prs.slides, 1):
            print(f"ğŸ¯ [EXTRACT-PPTX] Processing slide {i}/{total_slides}...")
            slide_text = []
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    slide_text.append(shape.text)
            slide_content = "\n".join(slide_text)
            text += slide_content + "\n\n"
            print(f"ğŸ¯ [EXTRACT-PPTX] Slide {i}: {len(slide_content)} chars extracted")
            
        print(f"ğŸ¯ [EXTRACT-PPTX] âœ… Complete! Total text: {len(text)} chars")
        print(f"ğŸ¯ [EXTRACT-PPTX] Preview: {text[:200]}...")
        return text
    except Exception as e:
        print(f"ğŸ¯ [EXTRACT-PPTX] âŒ ERROR: {e}")
        raise

def extract_text_from_file(file_path: str, filename: str) -> str:
    print(f"\nğŸ” [EXTRACT] Detecting file type for: {filename}")
    
    if filename.lower().endswith('.pdf'):
        print("ğŸ” [EXTRACT] Type detected: PDF")
        return extract_text_from_pdf(file_path)
    elif filename.lower().endswith('.docx'):
        print("ğŸ” [EXTRACT] Type detected: DOCX")
        return extract_text_from_docx(file_path)
    elif filename.lower().endswith('.pptx'):
        print("ğŸ” [EXTRACT] Type detected: PPTX")
        return extract_text_from_pptx(file_path)
    elif filename.lower().endswith('.txt'):
        print("ğŸ” [EXTRACT] Type detected: TXT")
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        print(f"ğŸ” [EXTRACT] TXT loaded: {len(text)} chars")
        return text
    else:
        print(f"ğŸ” [EXTRACT] âŒ Unsupported file type: {filename}")
        raise ValueError(f"Unsupported file type: {filename}")

# ========== SMART LLM-BASED CHUNKING ==========

def smart_topic_chunking(text: str, syllabus_topics: Optional[List[str]] = None) -> List[dict]:
    print("\n" + "="*80)
    print("ğŸ§  [CHUNKING] STARTING SMART TOPIC-BASED CHUNKING")
    print("="*80)
    print(f"ğŸ§  [CHUNKING] Input text length: {len(text)} chars")
    print(f"ğŸ§  [CHUNKING] Syllabus provided: {'âœ… YES' if syllabus_topics else 'âŒ NO'}")
    
    if syllabus_topics:
        print(f"ğŸ§  [CHUNKING] Number of syllabus topics: {len(syllabus_topics)}")
        for i, topic in enumerate(syllabus_topics, 1):
            print(f"ğŸ§  [CHUNKING]   {i}. {topic}")
        return chunk_by_syllabus(text, syllabus_topics)
    else:
        print("ğŸ§  [CHUNKING] Mode: AUTO-DISCOVERY (LLM will find topics)")
        return chunk_by_auto_discovery(text)

def chunk_by_syllabus(text: str, syllabus_topics: List[str]) -> List[dict]:
    print(f"\nğŸ“š [SYLLABUS-CHUNK] Syllabus-guided chunking started")
    print(f"ğŸ“š [SYLLABUS-CHUNK] Processing {len(syllabus_topics)} topics")
    
    topics_str = "\n".join([f"{i+1}. {t}" for i, t in enumerate(syllabus_topics)])
    print(f"ğŸ“š [SYLLABUS-CHUNK] Topics formatted:\n{topics_str}")
    
    matching_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a content analyzer. Given study material and syllabus topics, 
identify which sections of the material correspond to each topic.

Return a JSON array where each element has:
- topic: the syllabus topic name
- relevant_text: the extracted text section for this topic (can be multiple paragraphs)
- confidence: how confident you are this matches (0.0-1.0)

Only include topics that actually appear in the material. If a topic isn't covered, skip it."""),
        ("user", """Study Material:
{text}

Syllabus Topics:
{topics}

Return JSON only, no other text.""")
    ])
    
    print(f"ğŸ“š [SYLLABUS-CHUNK] Truncating text to 15000 chars for LLM...")
    text_truncated = text[:15000]
    print(f"ğŸ“š [SYLLABUS-CHUNK] Calling LLM for topic matching...")
    
    try:
        response = llm.invoke(matching_prompt.format_messages(text=text_truncated, topics=topics_str))
        print(f"ğŸ“š [SYLLABUS-CHUNK] âœ… LLM response received")
        print(f"ğŸ“š [SYLLABUS-CHUNK] Response length: {len(response.content)} chars")
        
        # Parse response
        content = response.content.strip()
        print(f"ğŸ“š [SYLLABUS-CHUNK] Parsing JSON response...")
        
        if content.startswith("```json"):
            print(f"ğŸ“š [SYLLABUS-CHUNK] Removing markdown code fence...")
            content = content[7:]
        if content.endswith("```"):
            content = content[:-3]
        
        chunks_data = json.loads(content.strip())
        print(f"ğŸ“š [SYLLABUS-CHUNK] âœ… JSON parsed successfully")
        print(f"ğŸ“š [SYLLABUS-CHUNK] Matched {len(chunks_data)} topics")
        
        chunks = []
        for i, chunk_data in enumerate(chunks_data):
            print(f"ğŸ“š [SYLLABUS-CHUNK] Processing matched topic {i+1}/{len(chunks_data)}...")
            chunk = {
                "chunk_id": str(uuid.uuid4()),
                "topic": chunk_data["topic"],
                "content": chunk_data["relevant_text"],
                "order": i
            }
            chunks.append(chunk)
            print(f"ğŸ“š [SYLLABUS-CHUNK]   âœ… Topic: {chunk['topic']}")
            print(f"ğŸ“š [SYLLABUS-CHUNK]   ğŸ“Š Content length: {len(chunk['content'])} chars")
            print(f"ğŸ“š [SYLLABUS-CHUNK]   ğŸ†” Chunk ID: {chunk['chunk_id']}")
        
        print(f"ğŸ“š [SYLLABUS-CHUNK] âœ… Syllabus chunking complete!")
        return chunks
        
    except Exception as e:
        print(f"ğŸ“š [SYLLABUS-CHUNK] âŒ ERROR: {e}")
        print(f"ğŸ“š [SYLLABUS-CHUNK] ğŸ”„ FALLBACK: Using auto-discovery instead...")
        return chunk_by_auto_discovery(text)

def chunk_by_auto_discovery(text: str) -> List[dict]:
    print(f"\nğŸ” [AUTO-CHUNK] Auto-discovery chunking started")
    
    discovery_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a content analyzer. Given study material, identify distinct topics/sections and split the content accordingly.

Return a JSON array where each element has:
- topic: a clear, descriptive topic name
- content: the text for this topic section
- order: numerical order (0, 1, 2, ...)

Look for natural topic boundaries like:
- Subject matter changes
- Conceptual shifts
- New chapters/sections
- Different themes

Aim for 5-15 meaningful chunks, not too granular."""),
        ("user", """Study Material:
{text}

Return JSON only, no other text.""")
    ])
    
    print(f"ğŸ” [AUTO-CHUNK] Truncating text to 15000 chars for LLM...")
    text_truncated = text[:15000]
    print(f"ğŸ” [AUTO-CHUNK] Calling LLM for topic discovery...")
    
    try:
        response = llm.invoke(discovery_prompt.format_messages(text=text_truncated))
        print(f"ğŸ” [AUTO-CHUNK] âœ… LLM response received")
        print(f"ğŸ” [AUTO-CHUNK] Response length: {len(response.content)} chars")
        
        # Parse response
        content = response.content.strip()
        print(f"ğŸ” [AUTO-CHUNK] Parsing JSON response...")
        
        if content.startswith("```json"):
            print(f"ğŸ” [AUTO-CHUNK] Removing markdown code fence...")
            content = content[7:]
        if content.endswith("```"):
            content = content[:-3]
        
        chunks_data = json.loads(content.strip())
        print(f"ğŸ” [AUTO-CHUNK] âœ… JSON parsed successfully")
        print(f"ğŸ” [AUTO-CHUNK] Discovered {len(chunks_data)} topics")
        
        chunks = []
        for chunk_data in chunks_data:
            chunk = {
                "chunk_id": str(uuid.uuid4()),
                "topic": chunk_data["topic"],
                "content": chunk_data["content"],
                "order": chunk_data.get("order", len(chunks))
            }
            chunks.append(chunk)
            print(f"ğŸ” [AUTO-CHUNK] Topic {chunk['order']}: {chunk['topic']}")
            print(f"ğŸ” [AUTO-CHUNK]   ğŸ“Š Content length: {len(chunk['content'])} chars")
            print(f"ğŸ” [AUTO-CHUNK]   ğŸ†” Chunk ID: {chunk['chunk_id']}")
        
        print(f"ğŸ” [AUTO-CHUNK] âœ… Auto-discovery chunking complete!")
        return chunks
        
    except Exception as e:
        print(f"ğŸ” [AUTO-CHUNK] âŒ ERROR: {e}")
        print(f"ğŸ” [AUTO-CHUNK] ğŸ”„ FALLBACK: Using simple character-based splitting...")
        
        # Fallback: simple splitting
        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        texts = splitter.split_text(text)
        print(f"ğŸ” [AUTO-CHUNK] Split into {len(texts)} chunks")
        
        chunks = []
        for i, chunk_text in enumerate(texts):
            chunks.append({
                "chunk_id": str(uuid.uuid4()),
                "topic": f"Section {i+1}",
                "content": chunk_text,
                "order": i
            })
            print(f"ğŸ” [AUTO-CHUNK] Fallback chunk {i+1}: {len(chunk_text)} chars")
        
        return chunks

# ========== SUMMARIZATION ==========

def generate_summary(chunk_content: str, topic: str) -> str:
    print(f"\nğŸ“ [SUMMARY] Generating summary")
    print(f"ğŸ“ [SUMMARY] Topic: {topic}")
    print(f"ğŸ“ [SUMMARY] Content length: {len(chunk_content)} chars")
    
    summary_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are Mr. Dexter, a master teacher. Create a clear, concise summary 
that helps students understand the key concepts.

Guidelines:
- 3-5 sentences max
- Focus on core concepts and key takeaways
- Use simple, clear language
- Help students grasp the essence quickly"""),
        ("user", """Topic: {topic}

Content:
{content}

Write a summary:""")
    ])
    
    print(f"ğŸ“ [SUMMARY] Truncating content to 3000 chars for LLM...")
    content_truncated = chunk_content[:3000]
    print(f"ğŸ“ [SUMMARY] Calling LLM for summary generation...")
    
    try:
        response = llm.invoke(summary_prompt.format_messages(topic=topic, content=content_truncated))
        summary = response.content.strip()
        print(f"ğŸ“ [SUMMARY] âœ… Summary generated: {len(summary)} chars")
        print(f"ğŸ“ [SUMMARY] Preview: {summary[:100]}...")
        return summary
    except Exception as e:
        print(f"ğŸ“ [SUMMARY] âŒ ERROR: {e}")
        print(f"ğŸ“ [SUMMARY] ğŸ”„ FALLBACK: Using truncated content as summary")
        return chunk_content[:300] + "..."

# ========== RAG SETUP ==========

def setup_rag_for_session(session_id: str, chunks: List[dict]) -> Chroma:
    print(f"\nğŸ—„ï¸ [RAG] Setting up RAG vector store")
    print(f"ğŸ—„ï¸ [RAG] Session ID: {session_id}")
    print(f"ğŸ—„ï¸ [RAG] Number of chunks: {len(chunks)}")
    
    documents = []
    for i, chunk in enumerate(chunks, 1):
        print(f"ğŸ—„ï¸ [RAG] Creating document {i}/{len(chunks)}...")
        doc = Document(
            page_content=chunk["content"],
            metadata={
                "chunk_id": chunk["chunk_id"],
                "topic": chunk["topic"],
                "order": chunk["order"],
                "summary": chunk.get("summary", "")
            }
        )
        documents.append(doc)
        print(f"ğŸ—„ï¸ [RAG]   âœ… Document created for: {chunk['topic']}")
    
    persist_dir = str(SESSIONS_DIR / session_id / "vectorstore")
    print(f"ğŸ—„ï¸ [RAG] Creating ChromaDB at: {persist_dir}")
    
    try:
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_dir
        )
        print(f"ğŸ—„ï¸ [RAG] âœ… Vector store created successfully!")
        print(f"ğŸ—„ï¸ [RAG] Persist directory: {persist_dir}")
        return vectorstore
    except Exception as e:
        print(f"ğŸ—„ï¸ [RAG] âŒ ERROR: {e}")
        raise

# ========== API ENDPOINTS ==========

@app.post("/api/sessions/create", response_model=SessionResponse)
async def create_session(
    files: List[UploadFile] = File(None),
    syllabus_text: Optional[str] = Form(None)
):
    print("\n" + "="*80)
    print("ğŸ“ [SESSION] CREATE SESSION REQUEST RECEIVED")
    print("="*80)
    
    session_id = str(uuid.uuid4())
    print(f"ğŸ“ [SESSION] Generated Session ID: {session_id}")
    
    session_dir = SESSIONS_DIR / session_id
    session_dir.mkdir(exist_ok=True)
    print(f"ğŸ“ [SESSION] Created session directory: {session_dir}")
    
    # Parse syllabus
    syllabus_topics = None
    if syllabus_text and syllabus_text.strip():
        print(f"ğŸ“š [SYLLABUS] Syllabus text received: {len(syllabus_text)} chars")
        print(f"ğŸ“š [SYLLABUS] Raw syllabus:\n{syllabus_text}")
        print(f"ğŸ“š [SYLLABUS] Calling LLM to extract topics...")
        
        syllabus_prompt = ChatPromptTemplate.from_messages([
            ("system", "Extract a clean list of topics from this syllabus. Return JSON array of strings."),
            ("user", "{syllabus}\n\nReturn JSON only.")
        ])
        
        try:
            response = llm.invoke(syllabus_prompt.format_messages(syllabus=syllabus_text))
            content = response.content.strip()
            print(f"ğŸ“š [SYLLABUS] LLM response received: {len(content)} chars")
            
            if content.startswith("```json"):
                content = content[7:-3]
            syllabus_topics = json.loads(content)
            print(f"ğŸ“š [SYLLABUS] âœ… Extracted {len(syllabus_topics)} topics:")
            for i, topic in enumerate(syllabus_topics, 1):
                print(f"ğŸ“š [SYLLABUS]   {i}. {topic}")
        except Exception as e:
            print(f"ğŸ“š [SYLLABUS] âŒ LLM parsing failed: {e}")
            print(f"ğŸ“š [SYLLABUS] ğŸ”„ FALLBACK: Using line-based splitting")
            syllabus_topics = [line.strip() for line in syllabus_text.split('\n') if line.strip()]
            print(f"ğŸ“š [SYLLABUS] Fallback extracted {len(syllabus_topics)} topics")
    else:
        print(f"ğŸ“š [SYLLABUS] No syllabus provided")
    
    # Process files
    combined_text = ""
    
    if files:
        print(f"ğŸ“¤ [UPLOAD] Processing {len(files)} uploaded file(s)")
        for i, file in enumerate(files, 1):
            if file.filename:
                print(f"ğŸ“¤ [UPLOAD] File {i}/{len(files)}: {file.filename}")
                file_path = UPLOAD_DIR / f"{session_id}_{file.filename}"
                
                print(f"ğŸ“¤ [UPLOAD] Saving to: {file_path}")
                with open(file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)
                print(f"ğŸ“¤ [UPLOAD] âœ… File saved")
                
                # Extract text
                text = extract_text_from_file(str(file_path), file.filename)
                combined_text += text + "\n\n"
                print(f"ğŸ“¤ [UPLOAD] Text extracted and added to combined text")
        
        print(f"ğŸ“¤ [UPLOAD] âœ… All files processed")
        print(f"ğŸ“¤ [UPLOAD] Total combined text: {len(combined_text)} chars")
    else:
        print(f"ğŸ“¤ [UPLOAD] No files uploaded")
    
    # Generate AI content if no files
    if not combined_text.strip():
        print("ğŸ¤– [AI-GEN] No content available, generating AI content...")
        if syllabus_topics:
            gen_prompt = f"Generate comprehensive study material for these topics:\n" + "\n".join(syllabus_topics)
            print(f"ğŸ¤– [AI-GEN] Using syllabus-based prompt")
        else:
            gen_prompt = "Generate sample educational content on a general topic."
            print(f"ğŸ¤– [AI-GEN] Using generic prompt")
        
        print(f"ğŸ¤– [AI-GEN] Calling LLM to generate content...")
        gen_response = llm.invoke(gen_prompt)
        combined_text = gen_response.content
        print(f"ğŸ¤– [AI-GEN] âœ… Generated {len(combined_text)} chars")
        print(f"ğŸ¤– [AI-GEN] Preview: {combined_text[:200]}...")
    
    # Smart chunking
    print(f"\nğŸ”„ [PIPELINE] Starting chunking pipeline...")
    chunks = smart_topic_chunking(combined_text, syllabus_topics)
    print(f"ğŸ”„ [PIPELINE] âœ… Chunking complete: {len(chunks)} chunks created")
    
    # Generate summaries
    print(f"\nğŸ”„ [PIPELINE] Starting summary generation for all chunks...")
    for i, chunk in enumerate(chunks, 1):
        print(f"ğŸ”„ [PIPELINE] Generating summary {i}/{len(chunks)}...")
        chunk["summary"] = generate_summary(chunk["content"], chunk["topic"])
    print(f"ğŸ”„ [PIPELINE] âœ… All summaries generated")
    
    # Setup RAG
    print(f"\nğŸ”„ [PIPELINE] Setting up RAG...")
    vectorstore = setup_rag_for_session(session_id, chunks)
    print(f"ğŸ”„ [PIPELINE] âœ… RAG setup complete")
    
    # Save session metadata
    session_data = {
        "session_id": session_id,
        "syllabus_topics": syllabus_topics,
        "chunks": chunks
    }
    
    metadata_path = session_dir / "metadata.json"
    print(f"ğŸ’¾ [SAVE] Saving session metadata to: {metadata_path}")
    with open(metadata_path, "w") as f:
        json.dump(session_data, f, indent=2)
    print(f"ğŸ’¾ [SAVE] âœ… Metadata saved")
    
    print("\n" + "="*80)
    print("ğŸ‰ [SESSION] SESSION CREATED SUCCESSFULLY!")
    print(f"ğŸ‰ [SESSION] Session ID: {session_id}")
    print(f"ğŸ‰ [SESSION] Total chunks: {len(chunks)}")
    print(f"ğŸ‰ [SESSION] Syllabus topics: {len(syllabus_topics) if syllabus_topics else 0}")
    print("="*80 + "\n")
    
    return SessionResponse(
        session_id=session_id,
        chunks=[ChunkResponse(**chunk) for chunk in chunks],
        syllabus_topics=syllabus_topics
    )

@app.get("/api/sessions/{session_id}", response_model=SessionResponse)
async def get_session(session_id: str):
    print(f"\nğŸ” [GET] Retrieving session: {session_id}")
    
    session_dir = SESSIONS_DIR / session_id
    metadata_file = session_dir / "metadata.json"
    
    print(f"ğŸ” [GET] Looking for metadata at: {metadata_file}")
    
    if not metadata_file.exists():
        print(f"ğŸ” [GET] âŒ Session not found!")
        raise HTTPException(status_code=404, detail="Session not found")
    
    print(f"ğŸ” [GET] âœ… Metadata file found, loading...")
    with open(metadata_file, "r") as f:
        session_data = json.load(f)
    
    print(f"ğŸ” [GET] âœ… Session loaded successfully")
    print(f"ğŸ” [GET] Chunks: {len(session_data['chunks'])}")
    print(f"ğŸ” [GET] Syllabus topics: {len(session_data.get('syllabus_topics', [])) if session_data.get('syllabus_topics') else 0}")
    
    return SessionResponse(
        session_id=session_data["session_id"],
        chunks=[ChunkResponse(**chunk) for chunk in session_data["chunks"]],
        syllabus_topics=session_data.get("syllabus_topics")
    )

@app.get("/api/health")
async def health_check():
    print("â¤ï¸ [HEALTH] Health check requested")
    print("â¤ï¸ [HEALTH] Status: healthy")
    return {"status": "healthy", "service": "Mr. Dexter API"}

print("\nâœ… All endpoints registered")
print("ğŸ“ POST /api/sessions/create - Create new session")
print("ğŸ“ GET /api/sessions/{session_id} - Get session")
print("ğŸ“ GET /api/health - Health check")

if __name__ == "__main__":
    import uvicorn
    print("\n" + "="*80)
    print("ğŸš€ STARTING MR. DEXTER API SERVER")
    print("="*80)
    print("ğŸ“ Server URL: http://localhost:8000")
    print("ğŸ“š API Docs: http://localhost:8000/docs")
    print("ğŸ”§ Interactive API: http://localhost:8000/redoc")
    print("="*80 + "\n")
    uvicorn.run(app, host="0.0.0.0", port=8000)